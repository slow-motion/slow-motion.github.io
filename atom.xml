<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://slow-motion.github.io</id>
    <title>Gridea</title>
    <updated>2021-06-15T08:24:28.268Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://slow-motion.github.io"/>
    <link rel="self" href="https://slow-motion.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://slow-motion.github.io/images/avatar.png</logo>
    <icon>https://slow-motion.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[软件工程大作业]]></title>
        <id>https://slow-motion.github.io/post/ruan-jian-gong-cheng-da-zuo-ye/</id>
        <link href="https://slow-motion.github.io/post/ruan-jian-gong-cheng-da-zuo-ye/">
        </link>
        <updated>2021-06-15T08:23:43.000Z</updated>
        <content type="html"><![CDATA[<h2 id="introduce">Introduce</h2>
<p>​	虽然 AlexNet 证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 在下面的几个章节中，我们将介绍一些常用于设计深层神经网络的启发式概念。</p>
<p>​	与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络结构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层次，现在又转向模块，重复各层的模式。</p>
<p>​	使用块的想法首先出现在牛津大学的 <a href="http://www.robots.ox.ac.uk/~vgg/">视觉几何组（visualgeometry Group）</a> (VGG) 的 VGG 网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的结构。</p>
<ul>
<li>
<p>VGG块</p>
<p>​	经典卷积神经网络的基本组成部分是下面的这个序列： 1. 带填充以保持分辨率的卷积层； 1. 非线性激活函数，如ReLU； 1. 池化层，如最大池化层。而一个 VGG 块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大池化层。在最初的 VGG 论文 [<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#simonyan-zisserman-2014">Simonyan &amp; Zisserman, 2014]</a> 中，作者使用了带有 3×3 卷积核、填充为 1（保持高度和宽度）的卷积层，和带有 2×2 池化窗口、步幅为 2（每个块后的分辨率减半）的最大池化层。在下面的代码中，我们定义了一个名为 <code>vgg_block</code> 的函数来实现一个 VGG 块。</p>
</li>
<li>
<p>VGG网络模型</p>
<p>​	与 AlexNet、LeNet 一样，VGG 网络可以分为两部分：第一部分主要由卷积层和池化层组成，第二部分由全连接层组成。如下图所示</p>
<figure data-type="image" tabindex="1"><a href="https://imgtu.com/i/2H0JVf"><img src="https://z3.ax1x.com/2021/06/15/2H0JVf.png" alt="2H0JVf.png" loading="lazy"></a></figure>
</li>
</ul>
<p>​	原始 VGG 网络有 5 个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第	一个模块有 64 个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到 512。由于该网	络使用 8 个卷积层和 3 个全连接层，因此它通常被称为 VGG-11。</p>
<p>​	总结如下：</p>
<ol>
<li>VGG-11 使用可复用的卷积块构造网络。不同的 VGG 模型可通过每个块中卷积层数量和输出通道数量的差异来定义。</li>
<li>块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。</li>
<li>在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即3×3）比较浅层且宽的卷积更有效。</li>
</ol>
<h2 id="how-to-use">How to use</h2>
<h3 id="数据集形式">数据集形式</h3>
<p>​	原始数据集存储形式为，同个类别的图像存储在同一个文件夹下，所有类别的图像存储在一个主文件夹data下。</p>
<blockquote>
<figure data-type="image" tabindex="2"><a href="https://imgtu.com/i/2Hc4hV"><img src="https://z3.ax1x.com/2021/06/15/2Hc4hV.png" alt="2Hc4hV.png" loading="lazy"></a></figure>
</blockquote>
<p>​	利用preprocess.py将数据集格式进行转换，转换后的数据集为，将训练集的路径与类别存储在train.txt文件中，测试机存储在val.txt中. 其中txt文件中的内容为</p>
<blockquote>
<figure data-type="image" tabindex="3"><a href="https://imgtu.com/i/2Hcjtx"><img src="https://z3.ax1x.com/2021/06/15/2Hcjtx.png" alt="2Hcjtx.png" loading="lazy"></a></figure>
</blockquote>
<p>​	随后使用train和test进行训练和测试，测试文件中有图形显示。</p>
]]></content>
    </entry>
</feed>